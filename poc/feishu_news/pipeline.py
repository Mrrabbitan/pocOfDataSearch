"""
AI æ–°é—» â†’ é£ä¹¦äº‘æ–‡æ¡£ å®Œæ•´ Pipeline

åŠŸèƒ½ï¼š
1. çˆ¬å–å¤šä¸ª AI ç§‘æŠ€æ–°é—»æº
2. è‡ªåŠ¨åˆ†ç±»ä¸å»é‡
3. æ ¼å¼åŒ–ä¸ºé£ä¹¦äº‘æ–‡æ¡£å¹¶å‘é€
4. æ”¯æŒå®šæ—¶è‡ªåŠ¨æ‰§è¡Œ
"""

import logging
import sys
from datetime import datetime
from collections import defaultdict

from news_crawler import crawl_ai_news, NewsArticle
from feishu_client import FeishuClient
from config import FEISHU_GROUP_CHAT_ID, FEISHU_GROUP_NAME

logger = logging.getLogger(__name__)


def _build_feishu_blocks(articles: list[NewsArticle], date_str: str) -> list[dict]:
    """å°†æ–°é—»åˆ—è¡¨è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£ block ç»“æ„"""
    blocks = []
    fc = FeishuClient.__new__(FeishuClient)  # ä»…ç”¨äºè°ƒç”¨ static æ–¹æ³•

    # â”€â”€ æ–‡æ¡£å¤´éƒ¨ â”€â”€
    blocks.append(fc.text_block(f"ğŸ“… æ—¥æœŸ: {date_str}  |  å…± {len(articles)} ç¯‡"))
    blocks.append(fc.text_block(f"â° ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"))
    blocks.append(fc.divider_block())

    # â”€â”€ æŒ‰åˆ†ç±»åˆ†ç»„ â”€â”€
    categorized: dict[str, list[NewsArticle]] = defaultdict(list)
    for a in articles:
        categorized[a.category].append(a)

    # åˆ†ç±»æ’åºæƒé‡
    category_order = [
        "ğŸ”¥ é‡å¤§å‘å¸ƒ",
        "ğŸ”¬ ç ”ç©¶çªç ´",
        "ğŸ’° äº§ä¸šåŠ¨æ€",
        "ğŸ› ï¸ å·¥å…·ä¸åº”ç”¨",
        "ğŸŒ æ”¿ç­–ä¸ä¼¦ç†",
        "ğŸ“° ç»¼åˆèµ„è®¯",
    ]

    for cat in category_order:
        cat_articles = categorized.get(cat, [])
        if not cat_articles:
            continue

        blocks.append(fc.heading_block(f"{cat} ({len(cat_articles)}ç¯‡)", level=2))

        for idx, article in enumerate(cat_articles, 1):
            # æ ‡é¢˜ (å¸¦é“¾æ¥)
            blocks.append(fc.heading_block(f"{idx}. {article.title}", level=3))

            # æ¥æºå’Œé“¾æ¥
            blocks.append(fc.link_block(f"ğŸ”— {article.source}: {article.url}", article.url))

            # æ‘˜è¦
            if article.summary:
                blocks.append(fc.text_block(f"ğŸ“ {article.summary}"))

            # å‘å¸ƒæ—¶é—´
            if article.published_at:
                blocks.append(fc.text_block(f"ğŸ“… å‘å¸ƒ: {article.published_at}"))

        blocks.append(fc.divider_block())

    # â”€â”€ æ–‡æ¡£å°¾éƒ¨ â”€â”€
    blocks.append(fc.heading_block("ğŸ¯ ä»Šæ—¥è¦ç‚¹", level=2))
    # å–å‰ 3 ç¯‡ä½œä¸ºè¦ç‚¹
    for i, a in enumerate(articles[:3], 1):
        blocks.append(fc.bullet_block(f"{a.title} ({a.source})"))

    blocks.append(fc.divider_block())
    blocks.append(fc.text_block("â€”â€” ç”± AI æ–°é—»èšåˆ Pipeline è‡ªåŠ¨ç”Ÿæˆ â€”â€”"))

    return blocks


def _build_group_text(articles: list[NewsArticle], doc_url: str, date_str: str) -> str:
    """æ„å»ºé£ä¹¦ç¾¤èŠæ¶ˆæ¯å†…å®¹"""
    lines = [
        f"ğŸ“° AI ç§‘æŠ€æ—¥æŠ¥ {date_str}",
        f"å…± {len(articles)} ç¯‡",
        f"æ–‡æ¡£é“¾æ¥: {doc_url}",
        "",
        "ä»Šæ—¥ç²¾é€‰ï¼š",
    ]
    for i, a in enumerate(articles[:5], 1):
        lines.append(f"{i}. {a.title}")
    return "\n".join(lines)


def run_pipeline(dry_run: bool = False) -> dict:
    """
    æ‰§è¡Œå®Œæ•´ pipeline:
    1. çˆ¬å– AI æ–°é—»
    2. åˆ›å»ºé£ä¹¦æ–‡æ¡£
    3. å†™å…¥å†…å®¹

    å‚æ•°:
        dry_run: å¦‚æœä¸º Trueï¼Œä»…çˆ¬å–æ–°é—»å¹¶æ‰“å°ï¼Œä¸å†™å…¥é£ä¹¦

    è¿”å›:
        {"status": "ok", "doc_url": "...", "article_count": N}
    """
    date_str = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œ AI æ–°é—» Pipeline â€” {date_str}")

    # 1. çˆ¬å–æ–°é—»
    logger.info("ğŸ“¡ Phase 1: çˆ¬å– AI ç§‘æŠ€æ–°é—»...")
    articles = crawl_ai_news()

    if not articles:
        logger.warning("âš ï¸  æœªçˆ¬å–åˆ°ä»»ä½•æ–°é—»ï¼ŒPipeline ç»ˆæ­¢")
        return {"status": "empty", "article_count": 0}

    logger.info(f"âœ… å…±è·å– {len(articles)} ç¯‡æ–°é—»")

    # 2. Dry Run æ¨¡å¼
    if dry_run:
        logger.info("\nğŸ“‹ [DRY RUN] æ–°é—»é¢„è§ˆ:")
        for i, a in enumerate(articles, 1):
            logger.info(f"  [{i}] [{a.category}] {a.title}")
            logger.info(f"      æ¥æº: {a.source}")
            logger.info(f"      é“¾æ¥: {a.url}")
            if a.summary:
                logger.info(f"      æ‘˜è¦: {a.summary[:80]}...")
        return {
            "status": "dry_run",
            "article_count": len(articles),
            "articles": [{"title": a.title, "url": a.url, "category": a.category} for a in articles],
        }

    # 3. åˆ›å»ºé£ä¹¦æ–‡æ¡£
    logger.info("ğŸ“„ Phase 2: åˆ›å»ºé£ä¹¦äº‘æ–‡æ¡£...")
    client = FeishuClient()
    doc_title = f"ğŸ“° AI ç§‘æŠ€æ—¥æŠ¥ â€” {date_str}"
    doc = client.create_document(doc_title)
    doc_id = doc["document_id"]
    doc_url = doc["url"]
    logger.info(f"   æ–‡æ¡£å·²åˆ›å»º: {doc_url}")

    # 4. å†™å…¥å†…å®¹
    logger.info("âœï¸  Phase 3: å†™å…¥æ–°é—»å†…å®¹...")
    blocks = _build_feishu_blocks(articles, date_str)

    # é£ä¹¦ API æ¯æ¬¡æœ€å¤šå†™å…¥ 50 ä¸ª blockï¼Œåˆ†æ‰¹å¤„ç†
    BATCH_SIZE = 50
    root_block_id = client.get_document_root_block(doc_id)

    for i in range(0, len(blocks), BATCH_SIZE):
        batch = blocks[i : i + BATCH_SIZE]
        client.write_blocks(doc_id, root_block_id, batch)
        logger.info(f"   å·²å†™å…¥ {min(i + BATCH_SIZE, len(blocks))}/{len(blocks)} blocks")

    # 5) å‘é€åˆ°é£ä¹¦ç¾¤èŠ
    logger.info("ğŸ“¨ Phase 4: å‘é€åˆ°é£ä¹¦ç¾¤èŠ...")
    group_result = {"status": "skipped"}
    chat_id = FEISHU_GROUP_CHAT_ID
    if not chat_id and FEISHU_GROUP_NAME:
        try:
            chat_id = client.find_chat_id_by_name(FEISHU_GROUP_NAME)
        except Exception as e:
            logger.warning(f"âš ï¸  è·å–ç¾¤èŠåˆ—è¡¨å¤±è´¥: {e}")
    if chat_id:
        group_text = _build_group_text(articles, doc_url, date_str)
        try:
            client.send_group_message(chat_id, group_text)
            group_result = {"status": "ok", "chat_id": chat_id}
            logger.info("âœ… å·²å‘é€åˆ°é£ä¹¦ç¾¤èŠ")
        except Exception as e:
            group_result = {"status": "error", "error": str(e)}
            logger.warning(f"âš ï¸  ç¾¤èŠå‘é€å¤±è´¥: {e}")
    else:
        logger.info("â„¹ï¸  æœªé…ç½®ç¾¤èŠ chat_idï¼Œæˆ–æ— æ³•é€šè¿‡ç¾¤åç§°æŸ¥æ‰¾")

    logger.info(f"\nğŸ‰ Pipeline å®Œæˆ!")
    logger.info(f"   ğŸ“„ æ–‡æ¡£: {doc_url}")
    logger.info(f"   ğŸ“° æ–°é—»æ•°: {len(articles)} ç¯‡")

    return {
        "status": "ok",
        "doc_url": doc_url,
        "document_id": doc_id,
        "article_count": len(articles),
        "group": group_result,
    }


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(message)s",
        datefmt="%H:%M:%S",
    )

    # æ”¯æŒ --dry-run å‚æ•°
    dry_run = "--dry-run" in sys.argv
    if dry_run:
        logger.info("ğŸƒ Dry Run æ¨¡å¼ â€” ä»…çˆ¬å–é¢„è§ˆï¼Œä¸å†™å…¥é£ä¹¦")

    result = run_pipeline(dry_run=dry_run)
    print(f"\n{'='*50}")
    print(f"Pipeline ç»“æœ: {result}")
