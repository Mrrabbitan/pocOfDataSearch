"""AI ç§‘æŠ€æ–°é—»çˆ¬å–æ¨¡å— â€”â€” ä»å¤šä¸ªæ¥æºèšåˆæœ€æ–° AI æ–°é—»"""

import re
import logging
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

from config import NEWS_SOURCES, SEARCH_QUERIES, NEWS_MAX_ARTICLES, NEWS_TODAY_ONLY

logger = logging.getLogger(__name__)

# è¯·æ±‚è¶…æ—¶ä¸ UA
TIMEOUT = 15
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/131.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
}


@dataclass
class NewsArticle:
    """æ–°é—»æ–‡ç« æ•°æ®ç»“æ„"""
    title: str
    url: str
    summary: str = ""
    source: str = ""
    category: str = ""
    published_at: Optional[str] = None
    tags: list = field(default_factory=list)

    @property
    def uid(self) -> str:
        """åŸºäº URL çš„å»é‡ ID"""
        return hashlib.md5(self.url.encode()).hexdigest()[:12]


# â”€â”€ ç½‘é¡µçˆ¬å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _fetch_page(url: str) -> Optional[BeautifulSoup]:
    """è·å–å¹¶è§£æç½‘é¡µ"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        resp.raise_for_status()
        return BeautifulSoup(resp.text, "html.parser")
    except Exception as e:
        logger.warning(f"âš ï¸  æŠ“å–å¤±è´¥ [{url}]: {e}")
        return None


def _extract_text(element, max_len: int = 300) -> str:
    """ä» HTML å…ƒç´ æå–çº¯æ–‡æœ¬"""
    if element is None:
        return ""
    text = element.get_text(strip=True)
    return text[:max_len] + "..." if len(text) > max_len else text


def _parse_datetime(text: str) -> Optional[datetime]:
    """è§£æå¸¸è§æ—¥æœŸæ ¼å¼ä¸º datetime"""
    if not text:
        return None
    raw = text.strip()
    if not raw:
        return None

    # ISO 8601
    candidate = raw.replace("Z", "+00:00")
    try:
        return datetime.fromisoformat(candidate)
    except Exception:
        pass

    # 2026-02-07 / 2026/02/07 / 2026.02.07
    m = re.search(r"(20\d{2})[./-](\d{1,2})[./-](\d{1,2})", raw)
    if m:
        return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))

    # 2026å¹´02æœˆ07æ—¥
    m = re.search(r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", raw)
    if m:
        return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))

    return None


def _extract_published_from_soup(soup: BeautifulSoup) -> str:
    """ä»æ–‡ç« é¡µæå–å‘å¸ƒæ—¶é—´"""
    meta_keys = [
        "article:published_time",
        "og:published_time",
        "publish_date",
        "pubdate",
        "date",
        "datePublished",
        "article:modified_time",
    ]
    for meta in soup.find_all("meta"):
        key = meta.get("property") or meta.get("name")
        if key in meta_keys:
            content = meta.get("content", "").strip()
            if content:
                return content

    time_el = soup.find("time")
    if time_el:
        value = time_el.get("datetime") or time_el.get("content") or time_el.get_text(strip=True)
        if value:
            return value

    return ""


def _enrich_published_at(article: NewsArticle) -> Optional[datetime]:
    """å°è¯•è¡¥å…¨æ–‡ç« å‘å¸ƒæ—¶é—´"""
    dt = _parse_datetime(article.published_at or "")
    if not dt:
        soup = _fetch_page(article.url)
        if soup:
            raw = _extract_published_from_soup(soup)
            dt = _parse_datetime(raw)
    if dt:
        article.published_at = dt.isoformat()
    return dt


def _is_today(dt: datetime) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå½“å¤©å†…å®¹ï¼ˆæœ¬åœ°æ—¶åŒºï¼‰"""
    if dt.tzinfo:
        dt = dt.astimezone()
    return dt.date() == datetime.now().date()


def _filter_today_articles(articles: list[NewsArticle]) -> list[NewsArticle]:
    """ä»…ä¿ç•™å½“å¤©æ–°é—»"""
    kept = []
    for article in articles:
        dt = _enrich_published_at(article)
        if not dt:
            continue
        if _is_today(dt):
            kept.append(article)
    logger.info(f"ğŸ§¹ ä»…ä¿ç•™å½“å¤©æ–°é—»: {len(kept)}/{len(articles)}")
    return kept


# â”€â”€ é€šç”¨æ–°é—»æå–å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_articles_generic(soup: BeautifulSoup, base_url: str, source_name: str) -> list[NewsArticle]:
    """é€šç”¨æ–‡ç« æå–ï¼šä»é¡µé¢ä¸­æ‰¾ <article> æˆ–å«é“¾æ¥çš„æ ‡é¢˜"""
    articles = []
    seen_urls = set()

    # ç­–ç•¥ 1: æŸ¥æ‰¾ <article> æ ‡ç­¾
    for article_el in soup.find_all("article", limit=20):
        link = article_el.find("a", href=True)
        if not link:
            continue
        href = urljoin(base_url, link["href"])
        if href in seen_urls or not _is_valid_article_url(href, base_url):
            continue
        seen_urls.add(href)

        title_el = article_el.find(["h1", "h2", "h3", "h4"])
        title = _extract_text(title_el) or _extract_text(link)
        if not title or len(title) < 5:
            continue

        summary_el = article_el.find("p")
        summary = _extract_text(summary_el, 200)

        time_el = article_el.find("time")
        pub_time = time_el.get("datetime", "") if time_el else ""

        articles.append(
            NewsArticle(
                title=title,
                url=href,
                summary=summary,
                source=source_name,
                published_at=pub_time,
            )
        )

    # ç­–ç•¥ 2: h2/h3 æ ‡é¢˜å†…é“¾æ¥ (è¡¥å……)
    if len(articles) < 3:
        for heading in soup.find_all(["h2", "h3"], limit=30):
            link = heading.find("a", href=True)
            if not link:
                continue
            href = urljoin(base_url, link["href"])
            if href in seen_urls or not _is_valid_article_url(href, base_url):
                continue
            seen_urls.add(href)

            title = _extract_text(heading)
            if not title or len(title) < 5:
                continue

            # å°è¯•æ‰¾ç›¸é‚» <p> ä½œä¸ºæ‘˜è¦
            next_p = heading.find_next_sibling("p")
            summary = _extract_text(next_p, 200) if next_p else ""

            articles.append(
                NewsArticle(title=title, url=href, summary=summary, source=source_name)
            )

    return articles


def _is_valid_article_url(url: str, base_url: str) -> bool:
    """è¿‡æ»¤æ‰éæ–‡ç« é“¾æ¥"""
    parsed = urlparse(url)
    path = parsed.path.lower()
    # æ’é™¤é¦–é¡µã€åˆ†ç±»é¡µã€æ ‡ç­¾é¡µç­‰
    skip_patterns = [
        "/category/", "/tag/", "/page/", "/author/",
        "/search", "/login", "/signup", "/about",
        "/contact", "/privacy", "/terms",
    ]
    if any(p in path for p in skip_patterns):
        return False
    # è‡³å°‘è¦æœ‰ä¸€çº§ä»¥ä¸Šçš„è·¯å¾„
    if path.count("/") < 2 and not path.endswith("/"):
        return True
    return len(path) > 5


# â”€â”€ ä¸­æ–‡æ–°é—»æºä¸“ç”¨æå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_jiqizhixin(soup: BeautifulSoup) -> list[NewsArticle]:
    """æœºå™¨ä¹‹å¿ƒä¸“ç”¨æå–"""
    articles = []
    for item in soup.select(".article-item, .article_item, .list-item", limit=15):
        link = item.find("a", href=True)
        if not link:
            continue
        href = urljoin("https://www.jiqizhixin.com", link["href"])
        title = _extract_text(item.find(["h2", "h3", "h4", ".title"]))
        if not title:
            title = _extract_text(link)
        summary = _extract_text(item.find(["p", ".summary", ".desc"]), 200)
        if title and len(title) > 4:
            articles.append(
                NewsArticle(title=title, url=href, summary=summary, source="æœºå™¨ä¹‹å¿ƒ")
            )
    return articles


def _extract_qbitai(soup: BeautifulSoup) -> list[NewsArticle]:
    """é‡å­ä½ä¸“ç”¨æå–"""
    articles = []
    for item in soup.select("article, .post-item, .news-item", limit=15):
        link = item.find("a", href=True)
        if not link:
            continue
        href = urljoin("https://www.qbitai.com", link["href"])
        title = _extract_text(item.find(["h2", "h3", "h4"]))
        if not title:
            title = _extract_text(link)
        summary = _extract_text(item.find("p"), 200)
        if title and len(title) > 4:
            articles.append(
                NewsArticle(title=title, url=href, summary=summary, source="é‡å­ä½")
            )
    return articles


# â”€â”€ æœç´¢å¼•æ“è¡¥å…… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _search_web_news(query: str) -> list[NewsArticle]:
    """é€šè¿‡ DuckDuckGo HTML æœç´¢è¡¥å……æ–°é—»"""
    articles = []
    try:
        search_url = f"https://html.duckduckgo.com/html/?q={requests.utils.quote(query)}"
        resp = requests.get(search_url, headers=HEADERS, timeout=TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        for result in soup.select(".result, .web-result", limit=10):
            link = result.find("a", class_="result__a", href=True)
            if not link:
                link = result.find("a", href=True)
            if not link:
                continue

            href = link.get("href", "")
            # DuckDuckGo æœ‰æ—¶ä¼šåŒ…è£… URL
            if "uddg=" in href:
                match = re.search(r"uddg=([^&]+)", href)
                if match:
                    href = requests.utils.unquote(match.group(1))

            title = _extract_text(link)
            snippet_el = result.find(class_="result__snippet")
            summary = _extract_text(snippet_el, 200) if snippet_el else ""

            if title and href.startswith("http"):
                articles.append(
                    NewsArticle(
                        title=title, url=href, summary=summary, source="Web Search"
                    )
                )
    except Exception as e:
        logger.warning(f"âš ï¸  æœç´¢å¤±è´¥ [{query}]: {e}")
    return articles


# â”€â”€ åˆ†ç±»å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CATEGORY_KEYWORDS = {
    "ğŸ”¥ é‡å¤§å‘å¸ƒ": [
        "launch", "release", "announce", "å‘å¸ƒ", "æ¨å‡º", "ä¸Šçº¿",
        "GPT", "Claude", "Gemini", "Llama", "DeepSeek",
    ],
    "ğŸ”¬ ç ”ç©¶çªç ´": [
        "research", "paper", "study", "breakthrough", "è®ºæ–‡",
        "ç ”ç©¶", "çªç ´", "benchmark", "SOTA",
    ],
    "ğŸ’° äº§ä¸šåŠ¨æ€": [
        "funding", "acquisition", "invest", "IPO", "èèµ„",
        "æ”¶è´­", "å¸‚åœº", "ä¼°å€¼", "partnership", "åˆä½œ",
    ],
    "ğŸ› ï¸ å·¥å…·ä¸åº”ç”¨": [
        "tool", "framework", "open source", "API", "SDK",
        "å¼€æº", "å·¥å…·", "åº”ç”¨", "plugin", "agent",
    ],
    "ğŸŒ æ”¿ç­–ä¸ä¼¦ç†": [
        "regulation", "policy", "safety", "ethic", "ç›‘ç®¡",
        "æ”¿ç­–", "å®‰å…¨", "ä¼¦ç†", "æ³•è§„",
    ],
}


def _categorize(article: NewsArticle) -> str:
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦è‡ªåŠ¨åˆ†ç±»"""
    text = f"{article.title} {article.summary}".lower()
    scores = {}
    for cat, keywords in CATEGORY_KEYWORDS.items():
        scores[cat] = sum(1 for kw in keywords if kw.lower() in text)
    best = max(scores, key=scores.get)
    return best if scores[best] > 0 else "ğŸ“° ç»¼åˆèµ„è®¯"


# â”€â”€ å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _deduplicate(articles: list[NewsArticle]) -> list[NewsArticle]:
    """åŸºäº URL hash å’Œæ ‡é¢˜ç›¸ä¼¼åº¦å»é‡"""
    seen_uids = set()
    seen_titles = set()
    unique = []
    for a in articles:
        if a.uid in seen_uids:
            continue
        # ç®€å•æ ‡é¢˜å»é‡ï¼šå‰ 20 å­—ç¬¦ç›¸åŒåˆ™è§†ä¸ºé‡å¤
        title_key = a.title[:20].lower().strip()
        if title_key in seen_titles:
            continue
        seen_uids.add(a.uid)
        seen_titles.add(title_key)
        unique.append(a)
    return unique


# â”€â”€ ä¸»å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def crawl_ai_news(max_articles: int = 0) -> list[NewsArticle]:
    """
    ä»å¤šä¸ªæ¥æºçˆ¬å– AI ç§‘æŠ€æ–°é—»
    è¿”å›å»é‡ã€åˆ†ç±»åçš„æ–‡ç« åˆ—è¡¨
    """
    max_articles = max_articles or NEWS_MAX_ARTICLES
    all_articles: list[NewsArticle] = []

    # 1) çˆ¬å–å„æ–°é—»æº
    for source in NEWS_SOURCES:
        logger.info(f"ğŸ” æ­£åœ¨æŠ“å–: {source['name']} ({source['url']})")
        soup = _fetch_page(source["url"])
        if not soup:
            continue

        if "jiqizhixin" in source["url"]:
            articles = _extract_jiqizhixin(soup)
        elif "qbitai" in source["url"]:
            articles = _extract_qbitai(soup)
        else:
            articles = _extract_articles_generic(soup, source["url"], source["name"])

        logger.info(f"   â†’ è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        all_articles.extend(articles)

    # 2) æœç´¢å¼•æ“è¡¥å……
    for query in SEARCH_QUERIES[:3]:  # é™åˆ¶æœç´¢æ¬¡æ•°
        logger.info(f"ğŸ” æœç´¢è¡¥å……: {query}")
        search_results = _search_web_news(query)
        all_articles.extend(search_results)

    # 3) å»é‡
    unique_articles = _deduplicate(all_articles)

    # 4) ä»…ä¿ç•™å½“å¤©
    if NEWS_TODAY_ONLY:
        unique_articles = _filter_today_articles(unique_articles)

    # 5) åˆ†ç±»
    for article in unique_articles:
        article.category = _categorize(article)

    # 6) æˆªæ–­
    result = unique_articles[:max_articles]
    logger.info(f"âœ… å…±è·å– {len(result)} ç¯‡å»é‡åçš„ AI æ–°é—»")
    return result


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    news = crawl_ai_news()
    for i, a in enumerate(news, 1):
        print(f"\n[{i}] [{a.category}] {a.title}")
        print(f"    æ¥æº: {a.source} | {a.url}")
        if a.summary:
            print(f"    æ‘˜è¦: {a.summary[:80]}...")
